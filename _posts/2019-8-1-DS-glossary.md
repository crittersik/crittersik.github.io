---
layout: post
title: Data Science Glossary
---

As part of my data science interview preparations, 
I've kept a list of concepts to refresh before interviews.
It's categorized into **Basic**, **Intermediate** and **Advanced**, 
with links to (mostly) respective Wikipedia article.

There are ML cheat-sheets or glossaries all over internet, 
some of them absolutely amazing.
I am just aiming here for a **list of concepts**, as complete as possible,
than one can use for revision learning.

Still working on the 'advanced' and 'very advanced' sections!

## Basic
* [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem)
* [bias and underfitting](https://en.wikipedia.org/wiki/Bias_of_an_estimator) 
* [bias-variance trade off](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)
* [classification](https://en.wikipedia.org/wiki/statistical_classification)
* [cross entropy](https://en.wikipedia.org/wiki/cross_entropy)
* [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))
* [decision tree](https://en.wikipedia.org/wiki/Decision_tree_learning)
* [gradient descent](https://youtu.be/F6GSRDoB-Cg)
* [k-nearest neighbours algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)
* [k-means algorithm](https://en.wikipedia.org/wiki/K-means_clustering)
* [linear regression](https://en.wikipedia.org/wiki/linear_regression)
* [logistic regression](https://youtu.be/-la3q9d7AKQ) (also [wiki](https://en.wikipedia.org/wiki/Logistic_regression))
* [loss function](https://youtu.be/yuH4iRcggMw)
* [MSE (mean squared error)](https://en.wikipedia.org/wiki/Mean_squared_error)
* [naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)
* [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution)
* [overfitting](https://en.wikipedia.org/wiki/Overfitting)
* [random forest algorithm](https://en.wikipedia.org/wiki/Random_forest) (also [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html))
* [regression](https://en.wikipedia.org/wiki/Regression_analysis)
* [regularization](https://www.youtube.com/watch?v=u73PU6Qwl1I) (also [wiki](https://en.wikipedia.org/wiki/Regularization_(mathematics)))
* [regularization for linear regression](https://www.youtube.com/watch?v=qbvRdrd0yJ8)
* [supervised learning](https://en.wikipedia.org/wiki/supervised_learning)
* [unsupervised learning](en.wikipedia.org/wiki/unsupervised_learning)
* [variance](https://en.wikipedia.org/wiki/Variance)
* [variance as overfitting error](https://en.wikipedia.org/wiki/Overfitting)

## Intermediate
* AUC (area under curve, area under the ROC curve)
* bagging
* [backpropagation](https://youtu.be/x_Eamf8MHwU)
* Bernoulli distribution
* [Binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution)
* boosting
* bootstrapping
* [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem)
* [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) (also [nonlinear](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction))
* Gaussian processes
* [gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting)
* [hypothesis testing](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing)
* [Kullback-Leiber divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence
* [neural network](https://youtu.be/1ZhtwInuOD0) (shallow, feed forward)
* [p-value](https://en.wikipedia.org/wiki/P-value)
* [perceptron](https://en.wikipedia.org/wiki/Perceptron)
* [PCA (principal component analysis)](https://en.wikipedia.org/wiki/Principal_component_analysis)
* [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution)
* [ROC (receiver operating characteristic)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)
* [selection bias](https://en.wikipedia.org/wiki/Selection_bias)
* [Simpson's paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox)
* [SVD (singular value decomposition)](https://en.wikipedia.org/wiki/Singular_value_decomposition)
* [SVM (support vector machine)](https://en.wikipedia.org/wiki/Support-vector_machine)
* [TF-IDF (term frequencyâ€“inverse document frequency)](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)

## Advanced
* [Bayesian network](https://en.wikipedia.org/wiki/Bayesian_network)
* Beta distribution
* [CNN](https://en.wikipedia.org/wiki/Convolutional_neural_network)
* [corpus (text)](https://en.wikipedia.org/wiki/Text_corpus)
* Gini index
* hidden Markov models
* [LDA (linear discriminant analysis)](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)
* [LSA (latent semantic analysis)](https://en.wikipedia.org/wiki/Latent_semantic_analysis)
* Markov chain
* Monte Carlo (Markov chain Monte Carlo)
* [n-gram](https://en.wikipedia.org/wiki/N-gram)
* [recommender system](https://en.wikipedia.org/wiki/Recommender_system)
* RNN - recurrent neural network
* [t-SNE (t-distributed stochastic neighbor embedding)](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)
* [word embedding](https://en.wikipedia.org/wiki/Word_embedding)
* [Word2Vec](https://en.wikipedia.org/wiki/Word2vec)
